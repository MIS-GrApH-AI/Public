{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-bruce",
   "metadata": {},
   "source": [
    "# Scraping the medical literature to add causal relationships among UMLS concepts in the graph database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-update",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-attack",
   "metadata": {},
   "source": [
    "![](images/Causal_expansion1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-appearance",
   "metadata": {},
   "source": [
    "The picture above illustrates one powerful use of causal relationships among diseases and their downstream effects. We can see a chain of causation that flows from the use of methamphatamines to death."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-little",
   "metadata": {},
   "source": [
    "![](images/Causal_expansion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-frederick",
   "metadata": {},
   "source": [
    "If we look at the longest path between methamphetamine use and death, we see the most detailed cause-effect chain known. In this example, there is an opportunity to intervene at each point in the cause-effect chain, and in a typical patient who is experiencing heart failure due to methamphetamine use, we do intervene at multiple points as shown here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-stanford",
   "metadata": {},
   "source": [
    "![](images/Concept_nodes_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-poker",
   "metadata": {},
   "source": [
    "The Unified Medical Language System (UMLS) has collected about 4.3 million medical concepts, which have been imported as nodes in our working group's graph database. There are some data sources which specify some relationships among these nodes, but so far we have not yet found any data source which shows direct <strong>causal</strong> relationships among them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-assets",
   "metadata": {},
   "source": [
    "## Mission\n",
    "<strong>Scrape the world's medical literature to find causal relationships among UMLS concepts and add the relationships to the graph.</strong>\n",
    "\n",
    "Example:\n",
    "\n",
    "Starting with an input string like this:  \n",
    ">'The autopsy showed no evidence of osteosarcoma, and the likely cause of death was cardiac failure with the evidence of pulmonary congestion, liver congestion, and multiple body cavity effusions.'  \n",
    "\n",
    "Do some of this magic:  \n",
    "![](images/sentence_parsing.svg)  \n",
    "Source: https://allenai.github.io/scispacy/\n",
    "\n",
    "And output a table that looks something like this:  \n",
    "\n",
    "|Concept_1|Relationship|Concept_2|Source PMID|  \n",
    "|---|---|---|---|  \n",
    "|cardiac failure|CAUSES|death|33554025|  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-surfing",
   "metadata": {},
   "source": [
    "## Helpful tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-thompson",
   "metadata": {},
   "source": [
    "### Access NCBI API to get causal strings\n",
    "\n",
    "The following endpoints are provided by the National Center for Biotechnology Information([NCBI](https://www.ncbi.nlm.nih.gov/))\n",
    "\n",
    "We'll be using the [Esearch](https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch) utility to get a list of publication ID numbers for articles containing a causal relationship of interest.\n",
    "\n",
    "We'll then use the [Efetch](https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.EFetch) utility to fetch articles identified by the previously identified IDs of interest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of databases availed by ncbi\n",
    "\n",
    "eutility_db_names = []\n",
    "db_names = []\n",
    "uid_common_names = []\n",
    "databases_url = 'https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly'\n",
    "databases_url_response = requests.get(databases_url)\n",
    "databases_url_content = databases_url_response.content\n",
    "\n",
    "databases_soup = BeautifulSoup(databases_url_content, 'html.parser')\n",
    "databases_table = databases_soup.select_one('#__chapter2\\.T\\._entrez_unique_identifiers_ui_lrgtbl__ > table:nth-child(1) > tbody:nth-child(2)')\n",
    "number_of_databases = len(databases_table.findAll('tr'))\n",
    "\n",
    "for row in range(1, number_of_databases+1):\n",
    "    db_name = databases_table.select_one('tr:nth-child({}) > td:nth-child(1)'.format(row))\n",
    "    db_names.append(db_name.text)\n",
    "    uid_common_name = databases_table.select_one('tr:nth-child({}) > td:nth-child(2)'.format(row))\n",
    "    uid_common_names.append(uid_common_name.text)\n",
    "    eutility_db_cell = databases_table.select_one('tr:nth-child({}) > td:nth-child(3)'.format(row))\n",
    "    eutility_db_names.append(eutility_db_cell.text)\n",
    "\n",
    "print('database names: {}'.format(db_names))\n",
    "print('e-utility database names: {}'.format(eutility_db_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-journalist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get the format for search query that can be passed into a URL, \n",
    "# perform an advanced search at pubmed, then copy what follows the &term= from that search's URL\n",
    "query = '(((((((cause[Title/Abstract]) NOT (all-cause[Title/Abstract])) ) ) ) OR (resulting in[Title/Abstract])) OR (due to[Title/Abstract])) AND (respiratory failure[Title/Abstract])'\n",
    "query = urllib.parse.quote(query, safe='') # Encode the query in URL format\n",
    "\n",
    "# databases to query. defaults to all ncbi databases\n",
    "databases_to_query = eutility_db_names\n",
    "\n",
    "# Get a list of PMIDs\n",
    "def get_list_ofIDs(query):\n",
    "    ids_dict = {}\n",
    "    for db_name in databases_to_query:\n",
    "        print('\\n{} database:'.format(db_names[databases_to_query.index(db_name)]))\n",
    "        esearch_query_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db={}&retmax=200&term={}'.format(db_name, query)\n",
    "        \n",
    "        print('search url: {}'.format(esearch_query_url))\n",
    "        response = requests.get(esearch_query_url)\n",
    "        content = response.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        try:\n",
    "            ids_str = soup.idlist.get_text()\n",
    "            ids_str = ids_str.replace('\\n',',')\n",
    "            ids_str = ids_str[1:-1]            \n",
    "        except AttributeError:\n",
    "            ids_str = None\n",
    "        print('IDs: {}'.format(ids_str))\n",
    "        if ids_str is None or ids_str.strip() == '': continue\n",
    "        ids_dict[db_name] = ids_str.split(',')\n",
    "    return ids_dict\n",
    "\n",
    "ids_dict = get_list_ofIDs(query)\n",
    "print(ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ba875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# material for retmode and rettype got from \n",
    "#     https://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.T._valid_values_of__retmode_and/?report=objectonly\n",
    "\n",
    "db_list = ids_dict.keys()\n",
    "\n",
    "def get_efetch_params(db_name):\n",
    "    return_mode = 'xml'\n",
    "    return_type = 'abstract'\n",
    "    text_node = 'abstract'\n",
    "    \n",
    "    if db_name == 'pubmed':\n",
    "        return_mode = 'xml'\n",
    "        return_type = 'abstract'\n",
    "        text_node = 'abstract'\n",
    "    elif db_name in ['bioproject','biosystems']:\n",
    "        return_type = 'xml'\n",
    "        return_mode = 'xml'\n",
    "    elif db_name in ['biosample', 'sra']:\n",
    "        return_type = 'full'\n",
    "        return_mode = 'xml'\n",
    "    elif db_name == 'gds':\n",
    "        return_type = 'summary'\n",
    "        return_mode = 'text'\n",
    "    elif db_name == 'mesh':\n",
    "        return_type = 'full'\n",
    "        return_mode = 'text'\n",
    "    elif db_name in ['nlmcatalog','sequences']:\n",
    "        return_type = None\n",
    "        return_mode = 'text'\n",
    "    elif db_name in ['nuccore','nucest','nucgss','protein','popset']:\n",
    "        return_type = 'native'\n",
    "        return_mode = 'xml'\n",
    "    elif db_name == 'pmc':\n",
    "        return_type = 'medline'\n",
    "        return_mode = 'text'\n",
    "        text_node = 'abstract'\n",
    "    elif db_name in ['taxonomy', 'gene']:\n",
    "        return_type = None\n",
    "        return_mode = 'xml'\n",
    "    elif db_name == 'gene':        \n",
    "        text_node = 'Entrezgene_summary'\n",
    "    elif db_name == 'snp':\n",
    "        return_type = 'docset'\n",
    "        return_mode = 'text'\n",
    "    elif db_name == 'clinvar':\n",
    "        return_type = 'clinvarset'\n",
    "        return_mode = 'xml'\n",
    "    elif db_name == 'gtr':\n",
    "        return_type = 'gtracc'\n",
    "        return_mode = 'xml'\n",
    "    else:\n",
    "        return_mode = None\n",
    "        \n",
    "    return (return_mode, return_type, text_node)\n",
    "    \n",
    "def get_article_content(db_name):\n",
    "    article_contents = {}\n",
    "    \n",
    "    return_mode, return_type, text_node = get_efetch_params(db_name)\n",
    "    \n",
    "    if return_mode is None: \n",
    "        print('{} database not supported by efetch utility'.format(db_name))\n",
    "        return None\n",
    "    \n",
    "    for article_id in ids_dict[db_name]:\n",
    "        print(\"article {} of {}\".format(ids_dict[db_name].index(article_id), len(ids_dict[db_name])))\n",
    "        efetch_params = 'rettype={}'.format(return_type)\n",
    "        if return_mode is not None: \n",
    "            efetch_params = efetch_params + '&retmode={}'.format(return_mode)\n",
    "        efetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db={}&id={}&{}'.format(db_name, article_id, efetch_params)\n",
    "        efetch_response = requests.get(efetch_url)\n",
    "        efetch_content = efetch_response.content\n",
    "        if return_mode == 'text': \n",
    "            article_text = efetch_content\n",
    "        else:\n",
    "            soup = BeautifulSoup(efetch_content, 'html.parser')\n",
    "            try:\n",
    "                article_text = soup.find(text_node).get_text()\n",
    "            except AttributeError:\n",
    "                print(\"no node {} at {}\".format(text_node, efetch_url))\n",
    "        article_contents[article_id] = article_text\n",
    "    \n",
    "    return article_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_of_interest(article_text):\n",
    "    causal_phrases = ['due to', 'cause', 'resulting in']\n",
    "    sentences_of_interest = []\n",
    "    for causal_phrase in causal_phrases:\n",
    "        regex = r\"([^.\\n]*?[^-]{}[^.]*\\.[^0-9])\".format(causal_phrase)\n",
    "        sentence_list = re.findall(regex, article_text)\n",
    "        sentences_of_interest = sentences_of_interest + sentence_list\n",
    "    return sentences_of_interest\n",
    "\n",
    "for db_name in db_list:\n",
    "    print(\"{} database:\".format(db_name))\n",
    "    article_contents = get_article_content(db_name)\n",
    "    if article_contents is None: continue\n",
    "    article_ids = article_contents.keys()\n",
    "    for article_id in article_ids:\n",
    "        strings_of_interest = extract_strings_of_interest(str(article_contents[article_id]))\n",
    "        print(\"strings of interest: article {} - {}\\n\".format(article_id, strings_of_interest))\n",
    "\n",
    "# Deal with negatives (e.g. \"this does not cause that\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-ottawa",
   "metadata": {},
   "source": [
    "### NLP toolkits\n",
    "\n",
    "Special thanks to Kevin Obuya for compiling this list:  \n",
    "https://docs.google.com/spreadsheets/d/13JADjvvbytmJCZ4l9IxmG8MblFYZYWME9vCldM_EKlA/edit?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
